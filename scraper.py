# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FkJLx-GBNFhfu0FybBkfVJKjbDnZ6HXz
"""

import requests
from bs4 import BeautifulSoup
import json
import time

headers = {'User-Agent': 'Mozilla/5.0'}

# ========== BBC News (English) ==========
def scrape_bbc_article(url):
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')
    try:
        title = soup.find('h1').get_text(strip=True)
        paragraphs = soup.find_all('div', attrs={'data-component': 'text-block'})
        body = '\n'.join([p.get_text(strip=True) for p in paragraphs])
    except:
        return None
    return {
        "title": title,
        "url": url,
        "content": body,
        "source": "BBC News"
    }

def get_bbc_article_links():
    base_url = 'https://www.bbc.com'
    news_url = base_url + '/news'
    response = requests.get(news_url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    links = set()
    for a_tag in soup.find_all('a', href=True):
        href = a_tag['href']
        if href.startswith('/news/articles/') and href.count('-') < 2:
            full_url = base_url + href
            links.add(full_url)
    return list(links)

# ========== The Guardian (English) ==========
def scrape_guardian_article(url):
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')
    try:
        title = soup.find('h1').get_text(strip=True)
        paragraphs = soup.select('div.article-body-commercial-selector p')
        body = '\n'.join([p.get_text(strip=True) for p in paragraphs])
    except:
        return None
    return {
        "title": title,
        "url": url,
        "content": body,
        "source": "The Guardian"
    }

def get_guardian_article_links():
    base_url = 'https://www.theguardian.com'
    news_url = base_url + '/international'
    response = requests.get(news_url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    links = set()
    for a_tag in soup.find_all('a', href=True):
        href = a_tag['href']
        if href.startswith('https://www.theguardian.com/') and href.count('/') > 4 and '/live/' not in href:
            links.add(href)
    return list(links)

# ========== Combined Scraper ==========
def scrape_latest_articles(limit_per_site=5):
    print("\nüîç Scraping from BBC News...")
    bbc_links = get_bbc_article_links()
    guardian_links = get_guardian_article_links()
    all_articles = []

    for i, url in enumerate(bbc_links[:limit_per_site]):
        print(f"  [BBC {i+1}/{limit_per_site}] {url}")
        article = scrape_bbc_article(url)
        if article:
            all_articles.append(article)
        time.sleep(1)

    print("\nüîç Scraping from The Guardian...")
    for i, url in enumerate(guardian_links[:limit_per_site]):
        print(f"  [Guardian {i+1}/{limit_per_site}] {url}")
        article = scrape_guardian_article(url)
        if article:
            all_articles.append(article)
        time.sleep(1)

    with open("bbc_guardian_articles.json", "w", encoding="utf-8") as f:
        json.dump(all_articles, f, ensure_ascii=False, indent=4)

    print(f"\n‚úÖ Saved {len(all_articles)} articles to bbc_guardian_articles.json")

# Run it
scrape_latest_articles(limit_per_site=5)
